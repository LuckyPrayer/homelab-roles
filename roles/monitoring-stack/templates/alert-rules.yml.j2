# Prometheus Alert Rules
# Generated by Ansible - Do not edit manually

groups:
  - name: homelab-alerts
    rules:
      # Host down alert
      - alert: HostDown
        expr: up == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Host {{ '{{' }} $labels.instance {{ '}}' }} is down"
          description: "{{ '{{' }} $labels.instance {{ '}}' }} has been unreachable for more than 2 minutes."

      # High CPU usage
      - alert: HighCpuUsage
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage on {{ '{{' }} $labels.instance {{ '}}' }}"
          description: "CPU usage is above 85% for more than 5 minutes on {{ '{{' }} $labels.instance {{ '}}' }}. Current: {{ '{{' }} $value | printf \"%.1f\" {{ '}}' }}%"

      # Critical CPU usage
      - alert: CriticalCpuUsage
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 95
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Critical CPU usage on {{ '{{' }} $labels.instance {{ '}}' }}"
          description: "CPU usage is above 95% for more than 2 minutes on {{ '{{' }} $labels.instance {{ '}}' }}. Current: {{ '{{' }} $value | printf \"%.1f\" {{ '}}' }}%"

      # High memory usage
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage on {{ '{{' }} $labels.instance {{ '}}' }}"
          description: "Memory usage is above 85% for more than 5 minutes on {{ '{{' }} $labels.instance {{ '}}' }}. Current: {{ '{{' }} $value | printf \"%.1f\" {{ '}}' }}%"

      # Critical memory usage
      - alert: CriticalMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 95
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Critical memory usage on {{ '{{' }} $labels.instance {{ '}}' }}"
          description: "Memory usage is above 95% on {{ '{{' }} $labels.instance {{ '}}' }}. Current: {{ '{{' }} $value | printf \"%.1f\" {{ '}}' }}%"

      # Disk space low
      - alert: DiskSpaceLow
        expr: (node_filesystem_avail_bytes{fstype=~"ext4|xfs"} / node_filesystem_size_bytes{fstype=~"ext4|xfs"}) * 100 < 15
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Low disk space on {{ '{{' }} $labels.instance {{ '}}' }}"
          description: "Disk {{ '{{' }} $labels.mountpoint {{ '}}' }} has less than 15% space remaining on {{ '{{' }} $labels.instance {{ '}}' }}. Free: {{ '{{' }} $value | printf \"%.1f\" {{ '}}' }}%"

      # Disk space critical
      - alert: DiskSpaceCritical
        expr: (node_filesystem_avail_bytes{fstype=~"ext4|xfs"} / node_filesystem_size_bytes{fstype=~"ext4|xfs"}) * 100 < 5
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Critical disk space on {{ '{{' }} $labels.instance {{ '}}' }}"
          description: "Disk {{ '{{' }} $labels.mountpoint {{ '}}' }} has less than 5% space remaining on {{ '{{' }} $labels.instance {{ '}}' }}!"

      # High network receive traffic
      - alert: HighNetworkReceive
        expr: sum by (instance) (rate(node_network_receive_bytes_total{device!~"lo|docker.*|br.*|veth.*"}[5m])) / 1024 / 1024 > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High network receive on {{ '{{' }} $labels.instance {{ '}}' }}"
          description: "Network receive rate is above 100 MB/s on {{ '{{' }} $labels.instance {{ '}}' }}. Current: {{ '{{' }} $value | printf \"%.1f\" {{ '}}' }} MB/s"

      # High network transmit traffic
      - alert: HighNetworkTransmit
        expr: sum by (instance) (rate(node_network_transmit_bytes_total{device!~"lo|docker.*|br.*|veth.*"}[5m])) / 1024 / 1024 > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High network transmit on {{ '{{' }} $labels.instance {{ '}}' }}"
          description: "Network transmit rate is above 100 MB/s on {{ '{{' }} $labels.instance {{ '}}' }}. Current: {{ '{{' }} $value | printf \"%.1f\" {{ '}}' }} MB/s"

      # Host clock skew
      - alert: HostClockSkew
        expr: abs(node_timex_offset_seconds) > 0.5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Clock skew detected on {{ '{{' }} $labels.instance {{ '}}' }}"
          description: "Host clock is out of sync by more than 500ms on {{ '{{' }} $labels.instance {{ '}}' }}. Offset: {{ '{{' }} $value | printf \"%.3f\" {{ '}}' }}s"

      # High load average
      - alert: HighLoadAverage
        expr: node_load15 / count by (instance) (node_cpu_seconds_total{mode="idle"}) > 2
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High load average on {{ '{{' }} $labels.instance {{ '}}' }}"
          description: "15-minute load average is high on {{ '{{' }} $labels.instance {{ '}}' }}. Current: {{ '{{' }} $value | printf \"%.2f\" {{ '}}' }}"

  - name: container-alerts
    rules:
      # Container stopped (not running)
      - alert: ContainerStopped
        expr: time() - container_last_seen{name!=""} > 60
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Container {{ '{{' }} $labels.name {{ '}}' }} has stopped"
          description: "Container {{ '{{' }} $labels.name {{ '}}' }} on {{ '{{' }} $labels.instance {{ '}}' }} has not been seen for more than 60 seconds."

      # Container high CPU
      - alert: ContainerHighCpu
        expr: (sum(rate(container_cpu_usage_seconds_total{name!=""}[5m])) by (name, instance) * 100) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Container {{ '{{' }} $labels.name {{ '}}' }} high CPU usage"
          description: "Container {{ '{{' }} $labels.name {{ '}}' }} CPU usage is above 80% for 5 minutes. Current: {{ '{{' }} $value | printf \"%.1f\" {{ '}}' }}%"

      # Container high memory  
      - alert: ContainerHighMemory
        expr: (container_memory_usage_bytes{name!=""} / container_spec_memory_limit_bytes{name!=""} * 100) > 85 and container_spec_memory_limit_bytes{name!=""} > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Container {{ '{{' }} $labels.name {{ '}}' }} high memory usage"
          description: "Container {{ '{{' }} $labels.name {{ '}}' }} is using more than 85% of its memory limit. Current: {{ '{{' }} $value | printf \"%.1f\" {{ '}}' }}%"

      # Container restart loop
      - alert: ContainerRestarting
        expr: increase(container_restart_count{name!=""}[15m]) > 3
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Container {{ '{{' }} $labels.name {{ '}}' }} restarting frequently"
          description: "Container {{ '{{' }} $labels.name {{ '}}' }} has restarted {{ '{{' }} $value | printf \"%.0f\" {{ '}}' }} times in the last 15 minutes."

      # Container OOM killed
      - alert: ContainerOomKilled
        expr: increase(container_oom_events_total{name!=""}[5m]) > 0
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "Container {{ '{{' }} $labels.name {{ '}}' }} was OOM killed"
          description: "Container {{ '{{' }} $labels.name {{ '}}' }} experienced an out-of-memory kill event."

  - name: service-alerts
    rules:
      # Prometheus target missing
      - alert: PrometheusTargetMissing
        expr: up == 0
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Prometheus target missing: {{ '{{' }} $labels.job {{ '}}' }}"
          description: "A Prometheus target has been down for more than 2 minutes. Job: {{ '{{' }} $labels.job {{ '}}' }}, Instance: {{ '{{' }} $labels.instance {{ '}}' }}"

      # Alertmanager notifications failing
      - alert: AlertmanagerNotificationsFailing
        expr: rate(alertmanager_notifications_failed_total[5m]) > 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Alertmanager notifications are failing"
          description: "Alertmanager is failing to send notifications. Check Alertmanager logs."

      # Loki ingestion rate
      - alert: LokiHighIngestionRate
        expr: sum(rate(loki_distributor_bytes_received_total[5m])) / 1024 / 1024 > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High Loki ingestion rate"
          description: "Loki is receiving logs at more than 10 MB/s. Current: {{ '{{' }} $value | printf \"%.1f\" {{ '}}' }} MB/s"

  - name: backup-alerts
    rules:
      # No backup in 24 hours (if you expose backup metrics)
      - alert: BackupOverdue
        expr: (time() - backup_last_success_timestamp_seconds) / 3600 > 26
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Backup overdue on {{ '{{' }} $labels.host {{ '}}' }}"
          description: "No successful backup in over 26 hours on {{ '{{' }} $labels.host {{ '}}' }}."
